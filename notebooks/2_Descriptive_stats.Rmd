---
title: "R Notebook"
output: html_notebook
---


#Part 2 - Descriptive statistics


Loading the cleaned dataset.

```{r message = FALSE, warning=FALSE}
library(readr)
library(magrittr)
library(stringi)
library(dplyr)

dataset_lang_eval_filtered <- read_csv(
  "../data/processed/dataset_lang_eval_filtered.csv",
  local = locale(encoding = "latin1")) %>%
  as.data.frame()
```

Do some basic descriptive statistics.

Which years are the articles from?
```{r}
table(dataset_lang_eval_filtered$date)


library(ggplot2)

ggplot(dataset_lang_eval_filtered, aes(date)) +
  geom_bar(fill = "#0073C2FF") 

png("../results/figures/year_plot.png")
year_plot <- textstat_frequency(text_dfm_clean, n = 50) %>%
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(x = "", y = "Term Frequency")+ theme_minimal()
print(year_plot)
dev.off()
```

Which organizations?
```{r}
table(dataset_lang_eval_filtered$org)


library(ggplot2)

ggplot(dataset_lang_eval_filtered, aes(org)) +
  geom_bar(fill = "#0073C2FF") 

png("../results/figures/organization_plot.png")
organization_plot <- textstat_frequency(text_dfm_clean, n = 50) %>%
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(x = "", y = "Term Frequency")+ theme_minimal()
print(organization_plot)
dev.off()
```

Which organizations are from which cities?
```{r}
org_city_table <- table(dataset_lang_eval_filtered$org,
                        dataset_lang_eval_filtered$city)


write.table(org_city_table, file = "../results/output/org_city_table.txt", sep = ",", quote = FALSE, row.names = T )
```
Get a descriptive statistics table to see the word counts and most published 
years:
```{r}
library(psych)

pub_date_table <- describe(dataset_lang_eval_filtered[,c(4,5)])

write.table(pub_date_table, file = "../results/output/pub_date_table.txt", sep = ",", quote = FALSE, row.names = T )
```

There are word counts already. Still, to check and 
explore this further - compute the number of words, the number of characters with 
and without punctuation, and the punctuation character counts:
```{r}

library(stringi)

n_words <- sapply(dataset_lang_eval_filtered$text_string, stri_count_words)
n_words <- unname(n_words)

char_punct <- sapply(dataset_lang_eval_filtered$text_string,stri_count_regex, "[^ ]")
char_punct <- unname(char_punct)

char_no_punct <- sapply(dataset_lang_eval_filtered$text_string,stri_count_regex, "[a-zA-Z0-9]")
char_no_punct <- unname(char_no_punct)

punct <- char_punct - char_no_punct



text_stats <- cbind.data.frame(identifier = dataset_lang_eval_filtered$identifier,
                    comment = dataset_lang_eval_filtered$text_string,
                    n_words,
                    char_no_punct,
                    char_punct,
                    punct)

text_stats$identifier <- as.character(text_stats$identifier)
text_stats$comment <- as.character(text_stats$comment)

write_csv(text_stats, "../results/output/text_stats.csv")
```
This confirms there is no punctuation in these texts. How do these
word counts compare to the ones available in the dataset?
```{r}
mean(text_stats$n_words)
mean(dataset_lang_eval_filtered$word_count)
```

Statistics on 
words and characters?
```{r}
words_char_calculated_table <- describe(text_stats[,c(3,4)])
write.table(words_char_calculated_table, file = "../results/output/words_char_calculated_table.txt", sep = ",", quote = FALSE, row.names = T )


describe(dataset_lang_eval_filtered$word_count)
```

Mean number of words in an article is 694.9, mean number of characters is
3251.92. There are differences in the dataset count and the count performed
here. 


Which words are most common?

First obtaining the tokens and unique number word count and the total word count.
In the process, symbols and numbers are removed.
```{r}
library(quanteda)

clean_tokens <- tokens(dataset_lang_eval_filtered$text_string, 
                       remove_punct = TRUE,
                       remove_symbols = TRUE,
                       remove_numbers = TRUE) %>% 
  as.character()


length(clean_tokens)
length(unique(clean_tokens))
```
There is a total of 701702 tokens and 39541 unique ones. (token ~= word in 
this case),

Now obtaining the most frequent words:
```{r}
text_dfm <- dfm(dataset_lang_eval_filtered$text_string,
                   remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE)

frequency_table <- textstat_frequency(text_dfm, n=20)

write_csv(frequency_table, "../results/output/frequency_table.csv")

```

Above one can see the frequency of each toke in the whole dataset and 
the number of documents it is present in. 

Drop some common words with less substantial meaning and check the top
words again:
```{r}
drop_list <- c("and", 
               "a", 
               "the", 
               "to", 
               "as", 
               "is", 
               "but", 
               "on", 
               "in",
               "of", 
               "that", 
               "for",
               "it",
               "are",
               "as",
               "was",
               "be")

text_dfm_clean <- text_dfm[,-which(colnames(text_dfm) %in% drop_list)]

frequency_table_clean <- textstat_frequency(text_dfm_clean, n=20)

write_csv(frequency_table_clean, "../results/output/frequency_table_clean.csv")

```


Plotting the top 50 words:
```{r}
top_50_words_plot <- textstat_frequency(text_dfm_clean, n = 50) %>%
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(x = "", y = "Term Frequency")+ theme_minimal()

png("../results/figures/top_50_words_plot.png")
top_50_words_plot <- textstat_frequency(text_dfm_clean, n = 50) %>%
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(x = "", y = "Term Frequency")+ theme_minimal()
print(top_50_words_plot)
dev.off()
```
