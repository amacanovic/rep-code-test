---
title: "R Notebook"
output: html_notebook
---

# Part 3 Testing topic modelling methods

Topic modelling is a method of unsupervised machine learning where
the algoritm clusters documents into so-called topics based on the frequencies
with which different words occur in them.
Here, we test the capabilities of a single method (PCA clustering) in separating
the texts coming from the first and second wave feminist organization texts.

###Simple insight - keyness analysis

Before starting, explore the dataset further - identify words which appear more
often in the first wave texts compared to the second wave and v.v.

Loading the file again:
```{r}
library(readr)
library(magrittr)
library(dplyr)

dataset_lang_eval_filtered <- read_csv(
  "../data/processed/dataset_lang_eval_filtered.csv",
  local = locale(encoding = "latin1")) %>%
  as.data.frame()
```


Creating a corpus, extracting the tokens and creating a DFM - a quanteda
version of DTM (DFM stands for a document feature matrix).
Quanteda uses this format for the keyness analysis.
```{r warning=F, message=F}
library(quanteda)

corpus_keyness <- corpus(dataset_lang_eval_filtered,
                         docid_field = "identifier",
                         text_field = "text_string")

tokens_keyness <- tokens(corpus_keyness,
                         remove_numbers = TRUE,
                         remove_punct = TRUE,
                         remove_symbols = TRUE,
                         remove_separators = TRUE)

```
Calculate the keyness using the first wave as the reference. This will
give an overview of words more often present in the first wave literature
than in the second wave literature:
```{r}
dfm_keyness <- dfm(tokens_keyness, groups = "wave")

keyness <- textstat_keyness(dfm_keyness,
                              target = "1",
                              measure = "lr")
```

Evaluate the results:
a. Words most representative of the first wave:
```{r}
first_wave <- keyness[order(-keyness$G2),]
print(first_wave[1:20])

write_csv(first_wave, "../results/output/first_wave_key_words.csv")
```

b. Words most representative of the second wave:
```{r}
second_wave <- keyness[order(keyness$G2),]
print(second_wave[1:20])

write_csv(second_wave, "../results/output/second_wave_key_words.csv")
```


###Building a word2vec model + kmean clustering

Build a word2vec model from this data. Word2vec is a method that represents
words as vectors in multidimensional space while preserving the context in which
they are situated. Thus, similar words will be closer in this space to each other.
Useful for exploring the datasets and also classification.


Loading the h2o library and loading the dataset into the library:
```{r message = FALSE, warning = FALSE}
library(h2o)
h2o.init(enable_assertions = FALSE)

text <- as.data.frame(dataset_lang_eval_filtered$text_string)
text <- dplyr::rename(text, text = "dataset_lang_eval_filtered$text_string")
text_h2o <- as.h2o(text)
```
Define the text cleanup and lemmatization function and tokenize the text:
```{r}
tokenize <- function(sentences) {
  tokenized <- h2o.tokenize(sentences, "\\\\W+")
  # convert to lower case
  tokenized_lower <- h2o.tolower(tokenized)
  # remove short words (less than 2 characters)
  tokenized_lengths <- h2o.nchar(tokenized_lower)
  tokenized_filtered <- tokenized_lower[is.na(tokenized_lengths) || tokenized_lengths >= 2, ]
  # remove words that contain numbers
  tokenized_words <- tokenized_filtered[h2o.grep("[0-9]", tokenized_filtered,
                                                 invert = TRUE,
                                                 output.logical = TRUE), ]
}

text_h2o$string <- h2o.ascharacter(text_h2o$text)
tokens_text <- tokenize(text_h2o$string)
```
Training the word2vec model and saving it: 
```{r}
w2v_model <- h2o.word2vec(tokens_text,
                          window_size = 5,
                          sent_sample_rate = 0,
                          epochs = 10)
model_path <- h2o.saveModel(object = w2v_model,
                            path="../data/temp",
							force = TRUE)

```
The model path is:
"reproducible-code-test-ana\\data\\temp\\Word2Vec_model_R_1601541182839_1"

To evaluate the performance of this model, check a few synonyms:
```{r}
synonym_woman <- h2o.findSynonyms(w2v_model, "woman", count = 10)
synonym_man <- h2o.findSynonyms(w2v_model, "man", count = 10)
synonym_work <- h2o.findSynonyms(w2v_model, "work", count = 10)

synonym_woman
synonym_man
synonym_work

write_csv(synonym_woman, "../results/output/word2vec_synonym_woman.csv")
write_csv(synonym_man, "../results/output/word2vec_synonym_man.csv")
write_csv(synonym_work, "../results/output/word2vec_synonym_work.csv")
```
Thhis model does not work well. This is because the order of the words
is not correct in this dataset, so it cannot rely on the context of the words.

Thus, this method will be skipped for now.


##Building an LDA model

LDA unsupervised machine learning model. 
LD takes a Document Term Matrix [DTM] as input. To obtain, using tm library 
functions.
```{r warning=FALSE}
library(tm)
```
To keep the IDs in the corpus and be able to trace back, set doc_id to 
global_id. Convert to a corpus.

```{r}
corpus_docs <- dataset_lang_eval_filtered %$%
  data.frame(doc_id = identifier,
             text = text_string,
             stringsAsFactors = FALSE) %>%
  DataframeSource() %>%
  Corpus()
```
Convertthe corpus into a document term matrix(where each row is a document, each
column is a word and values represent the frequency of each word occuring in each
document), removing punctuation and numbers.
```{r}
dtm_data <- DocumentTermMatrix(corpus_docs,
                                list(removePunctuation = TRUE,
                                     removeNumbers = TRUE))
dtm_data

```
Removing sparse terms is advised for computational and interpretability 
purposes.

To keep words appering in at least 100 documents, given the corpus size of
cca 32154 terms, the setting sparse needs to be set to: (32154-100)/32154, in this
case to 0.996 (for the purpose of the workshop. otherwise, work with more words)
```{r}
dtm_data <- removeSparseTerms(dtm_data, sparse = 0.996)

dtm_data
```
This removed some rarely occuring words, with 7568 terms remaining.

Removing any empty rows:
```{r}
ui <-  unique(dtm_data$i)
dtm_clean <-  dtm_data[ui, ]

dtm_matrix <- as.matrix(dtm_clean)
```

Computationally demanding code skipped. Test with a model
with 10 topics.


```{r}
library(topicmodels)

lda_10_topics <-  LDA(
  dtm_matrix,
  k = 10,
  method = "Gibbs",
  control = list(seed = 123, alpha = 50 / 10)
)
```


Obtaining topic assignment probabilities:
```{r}
lda_10_topics_gamma <- as.data.frame(lda_10_topics@gamma)
```


Check the top words in each topic:
```{r}
library(tidytext)

lda_topics_check <- tidy(lda_10_topics, matrix = "beta")


lda_top_terms <- lda_topics_check %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
  
  
lda_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
  
  
png("../results/figures/lda_top_terms.png")
lda_top_terms <- textstat_frequency(text_dfm_clean, n = 50) %>%
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(x = "", y = "Term Frequency")+ theme_minimal()
print(lda_top_terms)
dev.off()
```


It appears topic 2 could relate to the first wave and topic 10 to the second.
Using the previously obtained probabilities, assign everything above the mean 
probability of topic 10 to the first wave, everything below to the second wave:
```{r}
assign_documents_lda <- ifelse(
  lda_10_topics_gamma$V10 > mean(lda_10_topics_gamma$V10),
  "1",
  "2")

table(assign_documents_lda)
```

Now check this against the actual waves - 1 being the second wave and 0 being 
the first wave (need to recode for the measure)
```{r message=F, warning=F}
actual_data <- as.factor(ifelse(dataset_lang_eval_filtered$wave == "2", 1, 0))

predicted_data_lda <- as.factor(ifelse(assign_documents_lda == "2", 1, 0))
  

library(caret)

conf_matrix_lda <- confusionMatrix(actual_data,
                predicted_data_lda)

conf_matrix_lda$table
conf_matrix_lda$overall

write.table(conf_matrix_lda$table, "../results/output/conf_matrix_table.txt")
write.table(conf_matrix_lda$overall, "../results/output/conf_matrix_statistics.txt")

```
Accuracy of 52%. 
